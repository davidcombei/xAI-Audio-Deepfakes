import torch.nn as nn
import torch
from audioprocessor import AudioProcessor
import torch.nn.functional as F
import sys
from torch.autograd import Variable
import math
import numpy as np

audio_processor = AudioProcessor()


class Mask(nn.Module):
    def __init__(self, n_bands, hidden_size):
        super().__init__()
        self.linear1 = nn.Linear(n_bands, hidden_size)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_size, n_bands)
        self.freqs = torch.linspace(0, 8000/2, 1024//2 + 1)
        self.n_bands = n_bands
        self.band_edges = torch.linspace(0, self.freqs[-1], n_bands+1)
    def forward(self, bands_tensor):
        bands=bands_tensor.transpose(1,2)
        x = self.linear1(bands)
        x = self.relu(x)
        logits = self.linear2(x)
        mask = F.gumbel_softmax(logits, tau=0.5, hard=False, dim=-1)        
        masked_output = mask * bands
        masked_irrelevant_output = (1-mask) * bands

        ## frequency expandation -- chat GPT help here :)
        B, T, _ = bands.shape
        F_bins = self.freqs.shape[0]
        relevant_full = torch.zeros(B, T, F_bins, device=bands.device)
        irrelevant_full = torch.zeros_like(relevant_full)

        for i in range(self.n_bands):
            f_low, f_high = self.band_edges[i].item(), self.band_edges[i+1].item()
            idx = (self.freqs >= f_low) & (self.freqs < f_high)
            relevant_full[:, :, idx] = masked_bands[:, :, i:i+1]
            irrelevant_full[:, :, idx] = masked_irrelevant_bands[:, :, i:i+1]

        return relevant_full.transpose(1, 2), irrelevant_full.transpose(1, 2)





